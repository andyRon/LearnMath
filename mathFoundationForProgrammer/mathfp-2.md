## 19-概率和统计：编程为什么需要概率和统计？



### 1 概率和统计里有哪些需要掌握的概念？

**概率**（Probability），描述某种可能性的一个数值。

**随机变量**（Random Variable），描述事件所有可能出现的状态。分为**离散型随机变量**（Discrete Random Variable）和**连续型随机变量**（Continuous Random Variable）。

**概率分布**（Probability Distribution），描述每个状态出现的可能性。

例子：

假设我们使用一个随机变量 x 来表示新闻类型，如果在 100 篇新闻中，有 60 篇是娱乐新闻，有 20 篇是科技新闻，有 20 篇是体育新闻，那么你看到娱乐新闻的概率就是 60%，看到科技新闻的概率就是 20%，看到体育新闻的概率就是 20%。而这三组数据就可以构成变量 x 的**概率分布**P(x)。

如果添加另一个随机变量 y，表示新闻属于国际的还是国内的，那么新的概率分布就需要由 x 和 y 这两个变量联合起来才能决定，我们把这种概率称为**联合概率**（Joint Probability），可用P(x, y) 来表示。



对于离散型随机变量，通过联合概率 P(x, y) 在 y 上求和，就可以得到 P(x)，这个 P(x) 就是**边缘概率**（Marginal Probability）。

对于连续型随机变量，我们可以通过联合概率 P(x, y) 在 y 上的积分，推导出边缘概率 P(x)。

**边缘概率的作用**是<u>方便研究单个事件对概率分布的影响</u>。



**条件概率**，就是某个事件受其他事件影响之后出现的概率。

例子：

假设 100 篇中有 30 篇是国际新闻，而这 30 篇中有 5 篇是科技新闻，那么在国际新闻中出现科技新闻的概率就是 5/30=16.67%，在科技新闻中出现国际新闻的概率就是 5/20=25%。



其实**概率论研究的就是这些概率之间相互转化的关系，比如联合概率、条件概率和边缘概率**。通过这些关系，概率论中产生了著名的**贝叶斯定理**（Bayes’ theorem）。加上变量的独立性，我们就可以构建**朴素贝叶斯（Naive Bayes）分类算法**，这个算法在机器学习中的应用非常广泛。

基于概率发展而来的信息论，提出了很多重要的概率，例如**信息熵**（Entropy）/ **香农熵**（Shannon Entropy）、**信息增益**（Information Gain）、**基尼指数**（Gini）等。这些概念都被运用到了**决策树**（Decision Tree）的算法中。

<font color=#FF8C00>**概率**</font>和<font color=#FF8C00>**统计**</font>其实是互逆的。

概率论是<u>对数据产生的过程进行建模，然后研究某种模型所产生的数据有什么特性</u>。而统计学正好相反，它需要<u>通过已知的数据，来推导产生这些数据的模型是怎样的</u>。因此统计特别关注数据的各种分布、统计值及其对应的统计意义。

在真实的世界里，我们通常只能观测到一些数据，而无法事先知道，是什么模型产生了这些数据，这时候就要依赖统计学。所以，**海量数据的分析、实验和机器学习，都离不开统计学**。



### 2 概率和统计可以帮我们做什么？

之前计算平均复杂度的时候，都有一个前提假设：**所有情况出现的概率都是一样的**。

比如说，一个网站要对它的用户发放优惠券，那我们就需要先找到这些用户。我们用一个长度为 n 的数组代表某个网站的用户列表。我们假设第一个注册用户 ID 是 1，第二个注册用户的 ID 是 2，以此类推，最近刚刚注册的用户 ID 为 n。如果网站的发放策略是倾向于奖励新用户，那么被查找的用户 ID 有很大的概率会非常接近 n，因此平均复杂度就会非常接近 O(n)。相反，如果网站的发放策略是倾向于奖励老用户，那么搜索的用户 ID 有很大的概率是非常接近 1 的，因此平均复杂度会非常接近 O(1)。

你可以看到，现实中每种情况出现的可能性是不一样的，这也就意味者概率分布其是不均匀的。而不均匀的概率分布，最终会影响平均复杂度的加权平均计算。因此，要想获得更加准确的复杂度分析结果，我们必须要学习概率知识。

除此之外，**概率和统计对于机器学习和大数据分析而言更为重要**。对于机器学习而言，统计的运用是显而易见的。机器学习中的监督式学习，就是通过训练样本，估计出模型的参数，最后使用训练得出的模型，对新的数据进行预测。通过训练样本来估计模型，我们可以交给统计来完成。在机器学习的特征工程步骤中，我们可以使用统计的正态分布，**标准化**（standardization）不同取值范围的特征，让它们具有可比性。

此外，对机器学习算法进行效果评估时，<u>AB测试</u>可以减少不同因素对评测结果的干扰。为了获得更可靠的结论，我们需要理解统计意义，并为每个 AB 测试计算相应的统计值。

最后，**概率模型从理论上对某些机器学习算法提供了支持**。朴素贝叶斯分类充分利用了**贝叶斯定理**，使用先验概率推导出后验概率，并通过变量之间相互独立的假设，把复杂的计算进行大幅的简化。简化之后，我们就可以把这个算法运用在海量文本的分类任务上。

而决策树使用了**信息熵**和**信息增益**，挑出最具有区分力的条件，构建决策树的结点和分支。这样构建出的树，不仅分类效率更高，而且更利于人脑的理解。谷歌的**PageRank算法**利用**马尔科夫链**的概率转移，有效地刻画了人们浏览互联网的行为，大幅提升了互联网搜索的体验。

### 学习这部分书籍推荐



基础思想篇推荐书籍：

《离散数学及其应用》Kenneth H·Rosen



概率统计篇推荐书籍：

[《概率统计》](https://book.douban.com/subject/10827481/)  Morris H．DeGroot 和 Mark J．Schervish



线性代数篇推荐书籍：

[《线性代数及其应用》](https://book.douban.com/subject/1425950/)



入门、通识类书籍推荐

《程序员的数学》系列，包括《程序员的数学》《程序员的数学：概率统计》《程序员的数学：线性代数》

《数学之美》



